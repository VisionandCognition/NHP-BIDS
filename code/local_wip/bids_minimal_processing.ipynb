{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import errno\n",
    "\n",
    "import nipype.interfaces.io as nio           # Data i/o\n",
    "import nipype.interfaces.utility as niu      # utility\n",
    "import nipype.pipeline.engine as pe          # pypeline engine\n",
    "import nipype.algorithms.modelgen as model   # model generation\n",
    "import nipype.algorithms.rapidart as ra      # artifact detection\n",
    "from nipype.interfaces.utility import IdentityInterface\n",
    "\n",
    "from nipype.pipeline.engine import Workflow, Node, MapNode\n",
    "\n",
    "import nipype.interfaces.fsl as fsl          # fsl\n",
    "import nipype.interfaces.freesurfer as fs    # freesurfer\n",
    "\n",
    "# from bids_convert_csv_eventlog import ConvertCSVEventLog\n",
    "from subcode.bids_convert_csv_eventlog import ConvertCSVEventLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_images_workflow():\n",
    "    \"\"\" \n",
    "    Correct for sphinx position and use reorient to standard.\n",
    "    \"\"\"\n",
    "    workflow = Workflow(name='minimal_proc')\n",
    "    inputs = Node(IdentityInterface(\n",
    "        fields=['images']), name=\"in\")\n",
    "    outputs = Node(IdentityInterface(\n",
    "        fields=['images']), name=\"out\")\n",
    "\n",
    "    sphinx = MapNode(\n",
    "        fs.MRIConvert(sphinx=True),\n",
    "        iterfield=['in_file'], \n",
    "        name='sphinx')\n",
    "\n",
    "    workflow.connect(inputs, 'images',\n",
    "                     sphinx, 'in_file')\n",
    "\n",
    "    ro = MapNode(\n",
    "        fsl.Reorient2Std(),\n",
    "        iterfield=['in_file'],\n",
    "        name='ro')\n",
    "\n",
    "    workflow.connect(sphinx, 'out_file',\n",
    "                     ro, 'in_file')\n",
    "    workflow.connect(ro, 'out_file',\n",
    "                     outputs, 'images')\n",
    "\n",
    "    return workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_run(cmd):\n",
    "    print('%s\\n' % cmd)\n",
    "    return os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_functionals(raw_dir, glob_pat):\n",
    "    for fn in glob.glob(\"%s/%s\" % (raw_dir, glob_pat)):\n",
    "        fn = os.path.basename(fn)\n",
    "        print(\"Processing %s\" % fn)\n",
    "\n",
    "        print_run(\"mri_convert -i %s/%s -o /tmp/%s --sphinx\" %\n",
    "                  (raw_dir, fn, fn))\n",
    "        print_run(\"fslreorient2std /tmp/%s %s\" % (fn, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_workflow(session, csv_file, \n",
    "                 use_pbs, use_slurm, stop_on_first_crash,\n",
    "                 ignore_events, types):\n",
    "    import bids_templates as bt\n",
    "\n",
    "    from nipype import config\n",
    "    config.enable_debug_mode()\n",
    "\n",
    "    # ------------------ Specify variables\n",
    "    ds_root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n",
    "\n",
    "    data_dir = ds_root\n",
    "    output_dir = ''\n",
    "    working_dir = 'workingdirs/minimal_processing'\n",
    "\n",
    "    # ------------------ Input Files\n",
    "    infosource = Node(IdentityInterface(fields=[\n",
    "        'subject_id',\n",
    "        'session_id',\n",
    "    ]), name=\"infosource\")\n",
    "\n",
    "    reader = niu.CSVReader()\n",
    "    reader.inputs.header = True\n",
    "    reader.inputs.in_file = csv_file\n",
    "    out = reader.run()\n",
    "    subject_list = out.outputs.subject\n",
    "    session_list = out.outputs.session\n",
    "    infosource.iterables = [\n",
    "        ('session_id', session_list),\n",
    "        ('subject_id', subject_list),\n",
    "    ]\n",
    "    if 'run' in out.outputs.traits().keys():\n",
    "        print('Ignoring the \"run\" field of %s.' % csv_file)\n",
    "\n",
    "    infosource.synchronize = True\n",
    "\n",
    "    process_images = True\n",
    "\n",
    "    if process_images:\n",
    "        datatype_list = types.split(',')\n",
    "\n",
    "        imgsource = Node(IdentityInterface(fields=[\n",
    "            'subject_id', 'session_id', 'datatype',\n",
    "        ]), name=\"imgsource\")\n",
    "        imgsource.iterables = [\n",
    "            ('session_id', session_list), ('subject_id', subject_list),\n",
    "            ('datatype', datatype_list),\n",
    "        ]\n",
    "\n",
    "        # SelectFiles\n",
    "        imgfiles = Node(\n",
    "            nio.SelectFiles({\n",
    "                'images':\n",
    "                'sourcedata/%s' % bt.templates['images'],\n",
    "            }, base_directory=data_dir), name=\"img_files\")\n",
    "\n",
    "    if not ignore_events:  # only create an event node when handling events\n",
    "        evsource = Node(IdentityInterface(fields=[\n",
    "            'subject_id', 'session_id',\n",
    "        ]), name=\"evsource\")\n",
    "        evsource.iterables = [\n",
    "            ('session_id', session_list), ('subject_id', subject_list),\n",
    "        ]\n",
    "        evfiles = Node(\n",
    "            nio.SelectFiles({\n",
    "                'csv_eventlogs':\n",
    "                'sourcedata/sub-{subject_id}/ses-{session_id}/func/'\n",
    "                'sub-{subject_id}_ses-{session_id}_*events/Log_*_eventlog.csv',\n",
    "                'stim_dir':\n",
    "                'sourcedata/sub-{subject_id}/ses-{session_id}/func/'\n",
    "                'sub-{subject_id}_ses-{session_id}_*events/',\n",
    "            }, base_directory=data_dir), name=\"evfiles\")\n",
    "\n",
    "    # ------------------ Output Files\n",
    "    # Datasink\n",
    "    outputfiles = Node(nio.DataSink(\n",
    "        base_directory=ds_root,\n",
    "        container=output_dir,\n",
    "        parameterization=True),\n",
    "        name=\"output_files\")\n",
    "\n",
    "    # Use the following DataSink output substitutions\n",
    "    outputfiles.inputs.substitutions = [\n",
    "        ('subject_id_', 'sub-'),\n",
    "        ('session_id_', 'ses-'),\n",
    "        ('/minimal_processing/', '/'),\n",
    "        ('_out_reoriented.nii.gz', '.nii.gz')\n",
    "    ]\n",
    "    # Put result into a BIDS-like format\n",
    "    outputfiles.inputs.regexp_substitutions = [\n",
    "        (r'_datatype_([a-z]*)_ses-([a-zA-Z0-9]*)_sub-([a-zA-Z0-9]*)',\n",
    "            r'sub-\\3/ses-\\2/\\1'),\n",
    "        (r'/_ses-([a-zA-Z0-9]*)_sub-([a-zA-Z0-9]*)',\n",
    "            r'/sub-\\2/ses-\\1/'),\n",
    "        (r'/_ro[0-9]+/', r'/'),\n",
    "        (r'/_csv2tsv[0-9]+/', r'/func/'),\n",
    "    ]\n",
    "\n",
    "    # -------------------------------------------- Create Pipeline\n",
    "    workflow = Workflow(\n",
    "        name='wrapper',\n",
    "        base_dir=os.path.join(ds_root, working_dir))\n",
    "\n",
    "    if process_images:\n",
    "        workflow.connect([(imgsource, imgfiles,\n",
    "                           [('subject_id', 'subject_id'),\n",
    "                            ('session_id', 'session_id'),\n",
    "                            ('datatype', 'datatype'),\n",
    "                            ])])\n",
    "    if not ignore_events:\n",
    "        workflow.connect([(evsource, evfiles,\n",
    "                           [('subject_id', 'subject_id'),\n",
    "                            ('session_id', 'session_id'),\n",
    "                            ]),\n",
    "                          ])\n",
    "\n",
    "    if process_images:\n",
    "        minproc = create_images_workflow()\n",
    "        workflow.connect(imgfiles, 'images',\n",
    "                         minproc, 'in.images')\n",
    "        workflow.connect(minproc, 'out.images',\n",
    "                         outputfiles, 'minimal_processing.@images')\n",
    "\n",
    "    if not ignore_events:\n",
    "        csv2tsv = MapNode(\n",
    "            ConvertCSVEventLog(),\n",
    "            iterfield=['in_file', 'stim_dir'],\n",
    "            name='csv2tsv')\n",
    "        workflow.connect(evfiles, 'csv_eventlogs',\n",
    "                         csv2tsv, 'in_file')\n",
    "        workflow.connect(evfiles, 'stim_dir',\n",
    "                         csv2tsv, 'stim_dir')\n",
    "        workflow.connect(csv2tsv, 'out_file',\n",
    "                         outputfiles, 'minimal_processing.@eventlogs')\n",
    "\n",
    "    workflow.stop_on_first_crash = stop_on_first_crash\n",
    "    workflow.keep_inputs = True\n",
    "    workflow.remove_unnecessary_outputs = False\n",
    "    workflow.write_graph()\n",
    "    # workflow.run(plugin='MultiProc', plugin_args={'n_procs' : 10})\n",
    "    workflow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
